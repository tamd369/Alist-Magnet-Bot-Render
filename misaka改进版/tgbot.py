import requests
import sys
import re
import logging
import os
import asyncio
import ast
import math
import html
import urllib.parse
from datetime import datetime, timedelta

from dotenv import load_dotenv
from functools import wraps

from telegram import Update
from telegram.constants import ChatAction, ParseMode
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

# åŠ è½½.env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—ï¼ˆå¢å¼ºè¿‡æ»¤è§„åˆ™ï¼‰
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç¦æ­¢httpxçš„INFOçº§åˆ«æ—¥å¿—ï¼ˆè¿‡æ»¤HTTP/1.1 200 OKç­‰ä¿¡æ¯ï¼‰
httpx_logger = logging.getLogger("httpx")
httpx_logger.setLevel(logging.WARNING)  # ä»…æ˜¾ç¤ºWARNINGåŠä»¥ä¸Šçº§åˆ«æ—¥å¿—

# --- ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½® ---
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
BASE_URL = os.getenv("ALIST_BASE_URL")
USERNAME = os.getenv("ALIST_USERNAME")
PASSWORD = os.getenv("ALIST_PASSWORD")
OFFLINE_DOWNLOAD_DIR = os.getenv("ALIST_OFFLINE_DIR")
SEARCH_URL = os.getenv("JAV_SEARCH_API")
ALLOWED_USER_IDS_STR = os.getenv("ALLOWED_USER_IDS")
# æ–°å¢ï¼šä».env æ–‡ä»¶ä¸­åŠ è½½è‡ªåŠ¨æ¸…ç†é—´éš”æ—¶é—´
CLEAN_INTERVAL_MINUTES = int(os.getenv("CLEAN_INTERVAL_MINUTES", 60))
# æ–°å¢ï¼šä».env æ–‡ä»¶ä¸­åŠ è½½æ¸…ç†é˜ˆå€¼
SIZE_THRESHOLD = int(os.getenv("SIZE_THRESHOLD", 100)) * 1024 * 1024

# --- é…ç½®æ ¡éªŒ ---
if not all([TELEGRAM_TOKEN, BASE_URL, USERNAME, PASSWORD, OFFLINE_DOWNLOAD_DIR, SEARCH_URL, ALLOWED_USER_IDS_STR]):
    logger.error("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ç¼ºå¤±ï¼è¯·æ£€æŸ¥.env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è®¾ç½®ã€‚")
    sys.exit(1)

try:
    # å°†é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°é›†åˆ
    ALLOWED_USER_IDS = set(map(int, ALLOWED_USER_IDS_STR.split(',')))
    logger.info(f"å…è®¸çš„ç”¨æˆ· ID: {ALLOWED_USER_IDS}")
except ValueError:
    logger.error("é”™è¯¯: ALLOWED_USER_IDS æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®ä¿æ˜¯é€—å·åˆ†éš”çš„æ•°å­—ã€‚")
    sys.exit(1)

# --- å…¨å±€é…ç½® ---
TOKEN_EXPIRY_DURATION = timedelta(hours=24)  # Token æœ‰æ•ˆæœŸ 24 å°æ—¶

# --- ç”¨æˆ·æˆæƒè£…é¥°å™¨ ---
def restricted(func):
    @wraps(func)
    async def wrapped(update: Update, context: ContextTypes.DEFAULT_TYPE, *args, **kwargs):
        user_id = update.effective_user.id
        if user_id not in ALLOWED_USER_IDS:
            logger.warning(f"æœªæˆæƒç”¨æˆ·å°è¯•è®¿é—®: {user_id}")
            await update.message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
            return
        # æ£€æŸ¥å¹¶è·å– tokenï¼Œå­˜å‚¨åœ¨ bot_data ä¸­
        token = await get_token(context)
        if not token:
            await update.message.reply_text("é”™è¯¯: æ— æ³•è¿æ¥æˆ–ç™»å½•åˆ° Alist æœåŠ¡ã€‚")
            return
        # å°† token ä¼ é€’ç»™å¤„ç†å‡½æ•°
        return await func(update, context, token=token, *args, **kwargs)
    return wrapped

# --- API å‡½æ•° ---

def parse_size_to_bytes(size_str: str) -> int | None:
    """Converts size string (e.g., '5.40GB', '1.25MB') to bytes."""
    if not size_str:
        return 0  # Treat empty size as 0 bytes

    size_str = size_str.upper()
    match = re.match(r'^([\d.]+)\s*([KMGTPEZY]?B)$', size_str)
    if not match:
        logger.warning(f"æ— æ³•è§£ææ–‡ä»¶å¤§å°: {size_str}")
        return None  # Indicate parsing failure

    value, unit = match.groups()
    try:
        value = float(value)
    except ValueError:
        logger.warning(f"æ— æ³•è§£ææ–‡ä»¶å¤§å°å€¼: {value} from {size_str}")
        return None

    unit = unit.upper()
    exponent = 0
    if unit.startswith('K'):
        exponent = 1
    elif unit.startswith('M'):
        exponent = 2
    elif unit.startswith('G'):
        exponent = 3
    elif unit.startswith('T'):
        exponent = 4

    return int(value * (1024 ** exponent))

def parse_api_data_entry(entry_str: str) -> dict | None:
    """Parses a single string entry from the API data list."""
    try:
        data_list = ast.literal_eval(entry_str)
        if not isinstance(data_list, list) or len(data_list) < 4:
            logger.warning(f"è§£æåçš„æ•°æ®æ ¼å¼ä¸æ­£ç¡® (éåˆ—è¡¨æˆ–é•¿åº¦ä¸è¶³): {data_list}")
            return None

        magnet = data_list[0]
        name = data_list[1]
        size_str = data_list[2]
        date_str = data_list[3]

        if not magnet or not magnet.startswith("magnet:?"):
            logger.warning(f"æ¡ç›®ä¸­ç¼ºå°‘æœ‰æ•ˆçš„ç£åŠ›é“¾æ¥: {entry_str}")
            return None

        size_bytes = parse_size_to_bytes(size_str)
        if size_bytes is None:
            logger.warning(f"æ— æ³•è§£æå¤§å°ï¼Œè·³è¿‡æ¡ç›®: {entry_str}")
            return None

        upload_date = None
        try:
            if date_str:
                upload_date = datetime.strptime(date_str, '%Y-%m-%d').date()
        except ValueError:
            logger.warning(f"æ— æ³•è§£ææ—¥æœŸ '{date_str}'ï¼Œæ—¥æœŸå°†ä¸º None")

        return {
            "magnet": magnet,
            "name": name,
            "size_str": size_str,
            "size_bytes": size_bytes,
            "date_str": date_str,
            "date": upload_date,
            "original_string": entry_str
        }

    except (ValueError, SyntaxError, TypeError) as e:
        logger.error(f"è§£æ API æ•°æ®æ¡ç›®æ—¶å‡ºé”™: '{entry_str[:100]}...', é”™è¯¯: {e}")
        return None

def get_magnet(fanhao: str, search_url: str) -> tuple[str | None, str | None]:
    """è·å–ç£åŠ›é“¾æ¥ï¼ˆä¼˜åŒ–ç‰ˆç”¨æˆ·æç¤ºï¼‰"""
    try:
        url = search_url.rstrip('/') + "/" + fanhao
        logger.info(f"æ­£åœ¨æœç´¢ç•ªå·: {fanhao}")
        response = requests.get(url, timeout=20)  # æ˜ç¡®å®šä¹‰ response
        response.raise_for_status()

        raw_result = response.json()

        # --- å¤„ç†ä¸šåŠ¡é€»è¾‘é”™è¯¯ ---
        if not raw_result or raw_result.get("status") != "succeed":
            error_type = raw_result.get('message', 'æœªçŸ¥é”™è¯¯')
            if "not found" in error_type.lower():
                return None, f"ğŸ” æœªæ‰¾åˆ°ç•ªå· {fanhao} ç›¸å…³èµ„æº"
            return None, f"ğŸ” æœç´¢æœåŠ¡å¼‚å¸¸ ({error_type[:20]}...)"

        if not raw_result.get("data") or len(raw_result["data"]) == 0:
            return None, f"ğŸ” ç•ªå· {fanhao} æš‚æ— æœ‰æ•ˆç£åŠ›"

        # --- è§£ææ•°æ®æ¡ç›® ---
        parsed_entries = []
        for entry_str in raw_result["data"]:
            parsed = parse_api_data_entry(entry_str)
            if parsed and parsed["magnet"].startswith("magnet:?"):
                parsed_entries.append(parsed)

        if not parsed_entries:
            return None, f"ğŸ” æ‰¾åˆ°èµ„æºä½†æ— æœ‰æ•ˆç£åŠ›"

        # --- æ™ºèƒ½é€‰æ‹©é€»è¾‘ï¼ˆä¿æŒåŸæ ·ï¼‰---
        max_size = max(e["size_bytes"] for e in parsed_entries)
        hd_threshold = max_size * 0.7
        selected_cluster = [e for e in parsed_entries if e["size_bytes"] >= hd_threshold] or parsed_entries

        selected_cluster.sort(
            key=lambda x: (x["size_bytes"],
                         -(x["date"].toordinal() if x["date"] else 0)))

        return selected_cluster[0]["magnet"], None

    # --- å¼‚å¸¸å¤„ç†ï¼ˆä¼˜åŒ–æç¤ºï¼‰---
    except requests.exceptions.Timeout:
        logger.error(f"æœç´¢è¶…æ—¶ ({fanhao})")
        return None, "â³ æœç´¢è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"

    except requests.exceptions.HTTPError as e:
        # ç¡®ä¿ e.response å­˜åœ¨
        status_code = e.response.status_code  # æ­£ç¡®å¼•ç”¨ e.response
        if 500 <= status_code < 600 or status_code == 404:
            return None, f"ğŸ” ç•ªå· {fanhao} ä¸å­˜åœ¨"
        return None, f"ğŸ” æœç´¢æœåŠ¡å¼‚å¸¸ (HTTP {status_code})"

    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯ ({fanhao}): {str(e)}", exc_info=True)
        if "timed out" in str(e).lower():
            return None, "â³ æ“ä½œè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        return None, "ğŸ” æœç´¢æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯"

async def get_token(context: ContextTypes.DEFAULT_TYPE) -> str | None:
    """è·å– Alist Tokenï¼Œå¸¦æœ‰æ•ˆæœŸç¼“å­˜"""
    bot_data = context.bot_data
    token = bot_data.get("alist_token")
    token_expiry = bot_data.get("token_expiry")

    if token and token_expiry and datetime.now() < token_expiry:
        logger.info("ä½¿ç”¨æœ‰æ•ˆç¼“å­˜çš„ Alist token")
        return token

    try:
        url = BASE_URL.rstrip('/') + "/api/auth/login"
        logger.info("ç¼“å­˜ token æ— æ•ˆæˆ–è¿‡æœŸï¼Œæ­£åœ¨é‡æ–°è·å–...")
        login_info = {"username": USERNAME, "password": PASSWORD}
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(url, json=login_info, timeout=15)
        )
        response.raise_for_status()

        result = response.json()
        if result.get("code") == 200 and result.get("data") and result["data"].get("token"):
            token = str(result['data']['token'])
            bot_data["alist_token"] = token
            bot_data["token_expiry"] = datetime.now() + TOKEN_EXPIRY_DURATION
            logger.info("æˆåŠŸè·å–å¹¶ç¼“å­˜æ–°çš„ Alist token")
            return token
        else:
            error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯')
            logger.error(f"Alist ç™»å½•å¤±è´¥: {error_msg} (Code: {result.get('code', 'N/A')})")
            return None
    except requests.exceptions.RequestException as e:
        logger.error(f"ç™»å½• Alist è·å– token æ—¶å‡ºé”™: {str(e)}")
        return None
    except Exception as e:
        logger.error(f"ç™»å½• Alist è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}", exc_info=True)
        return None

async def add_magnet(context: ContextTypes.DEFAULT_TYPE, token: str, magnet: str) -> tuple[bool, str]:
    """è¿”å›æ ¼å¼ï¼š(æ˜¯å¦æˆåŠŸ, ç»“æœæè¿°)"""
    if not token or not magnet:
        logger.error("æ·»åŠ ä»»åŠ¡å¤±è´¥: token æˆ–ç£åŠ›é“¾æ¥ä¸ºç©º")
        return False, "âŒ å†…éƒ¨é”™è¯¯ï¼šå¿…è¦å‚æ•°ç¼ºå¤±"

    try:
        url = BASE_URL.rstrip('/') + "/api/fs/add_offline_download"
        headers = {
            "Authorization": token,
            "Content-Type": "application/json"
        }
        post_data = {
            "path": OFFLINE_DOWNLOAD_DIR,
            "urls": [magnet],
            "tool": "storage",
            "delete_policy": "delete_on_upload_succeed"
        }

        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(url, json=post_data, headers=headers, timeout=30))

        # å¤„ç†å·²çŸ¥é”™è¯¯çŠ¶æ€
        if response.status_code == 401:
            context.bot_data.pop("alist_token", None)
            return False, "âŒ è®¤è¯è¿‡æœŸï¼Œè¯·é‡è¯•"
        if response.status_code == 500:
            return False, "âŒ æœåŠ¡å™¨æ‹’ç»è¯·æ±‚ï¼ˆå¯èƒ½é‡å¤æ·»åŠ ï¼‰"

        response.raise_for_status()
        result = response.json()

        if result.get("code") == 200:
            return True, "âœ… å·²æ·»åŠ è‡³ä¸‹è½½é˜Ÿåˆ—"
        return False, f"âŒ ç£åŠ›è§£æå¤±è´¥"

    except requests.exceptions.Timeout:
        return False, "â³ æ·»åŠ è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ"
    except requests.exceptions.ConnectionError:
        return False, "ğŸ”Œ æ— æ³•è¿æ¥AlistæœåŠ¡"
    except Exception as e:
        logger.error(f"æ·»åŠ ä»»åŠ¡å¼‚å¸¸: {str(e)}")
        return False, f"âŒ æ„å¤–é”™è¯¯: {str(e)[:50]}"

async def recursive_collect_files(token: str, base_url: str, current_path: str) -> list[str]:
    """é€’å½’æ”¶é›†ç›®å½•ä¸‹æ‰€æœ‰å°æ–‡ä»¶ï¼ˆè¿”å›ç»å¯¹è·¯å¾„ï¼‰"""
    if SIZE_THRESHOLD == 0:
        return []
    list_url = base_url.rstrip('/') + "/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}
    payload = {"path": current_path, "page": 1, "per_page": 0}
    files = []

    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(list_url, json=payload, headers=headers, timeout=20)
        )
        response.raise_for_status()
        list_result = response.json()

        # é˜²å¾¡æ€§æ•°æ®è§£æ
        data = list_result.get("data") or {}
        if list_result.get("code") != 200:
            logger.error(f"ç›®å½•åˆ—è¡¨å¤±è´¥: {list_result.get('message')} (è·¯å¾„: {current_path})")
            return []

        content = data.get("content") or []
        if not isinstance(content, list):
            logger.error(f"æ— æ•ˆçš„APIå“åº”æ ¼å¼ (è·¯å¾„: {current_path})")
            return []

        for item in content:
            # å…³é”®è·¯å¾„å¤„ç†é€»è¾‘
            try:
                is_dir = item.get("is_dir", False)
                file_name = item.get("name", "").strip()
                file_size = item.get("size", 0)

                if not file_name:
                    continue

                # æ„å»ºæ ‡å‡†åŒ–ç»å¯¹è·¯å¾„ï¼ˆå…¼å®¹Windows/Linuxï¼‰
                full_path = "/".join([current_path.rstrip("/"), file_name.lstrip("/")])

                if is_dir:
                    # é€’å½’å¤„ç†ç›®å½•
                    sub_files = await recursive_collect_files(token, base_url, full_path)
                    files.extend(sub_files)
                else:
                    # åªæ”¶é›†å°äºé˜ˆå€¼æ–‡ä»¶
                    if file_size < SIZE_THRESHOLD:
                        files.append(full_path)
                        logger.debug(f"æ‰¾åˆ°å€™é€‰æ–‡ä»¶: {full_path} ({file_size/1024/1024:.2f} MB)")

            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶é¡¹æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                continue

        return files

    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)} (è·¯å¾„: {current_path})")
        return []
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)} (è·¯å¾„: {current_path})", exc_info=True)
        return []

async def recursive_collect_empty_dirs(token: str, base_url: str, current_path: str) -> list[str]:
    """é€’å½’æ”¶é›†ç›®å½•ä¸‹æ‰€æœ‰ç©ºæ–‡ä»¶å¤¹ï¼ˆè¿”å›ç»å¯¹è·¯å¾„ï¼‰"""
    list_url = base_url.rstrip('/') + "/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}
    payload = {"path": current_path, "page": 1, "per_page": 0}
    empty_dirs = []

    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(list_url, json=payload, headers=headers, timeout=20)
        )
        response.raise_for_status()
        list_result = response.json()

        # é˜²å¾¡æ€§æ•°æ®è§£æ
        data = list_result.get("data") or {}
        if list_result.get("code") != 200:
            logger.error(f"ç›®å½•åˆ—è¡¨å¤±è´¥: {list_result.get('message')} (è·¯å¾„: {current_path})")
            return []

        content = data.get("content") or []
        if not isinstance(content, list):
            logger.error(f"æ— æ•ˆçš„APIå“åº”æ ¼å¼ (è·¯å¾„: {current_path})")
            return []

        sub_dirs = []
        for item in content:
            is_dir = item.get("is_dir", False)
            file_name = item.get("name", "").strip()
            if not file_name:
                continue
            full_path = "/".join([current_path.rstrip("/"), file_name.lstrip("/")])
            if is_dir:
                sub_dirs.append(full_path)
        for sub_dir in sub_dirs:
            sub_empty_dirs = await recursive_collect_empty_dirs(token, base_url, sub_dir)
            empty_dirs.extend(sub_empty_dirs)

        if not sub_dirs and not any(not item.get("is_dir", False) for item in content):
            empty_dirs.append(current_path)

        return empty_dirs

    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)} (è·¯å¾„: {current_path})")
        return []
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {str(e)} (è·¯å¾„: {current_path})", exc_info=True)
        return []


async def cleanup_empty_dirs(token: str, base_url: str, target_dir: str) -> tuple[int, str]:
    """æ¸…ç†ç©ºæ–‡ä»¶å¤¹å¹¶è¿”å›æˆåŠŸåˆ é™¤çš„æ–‡ä»¶å¤¹æ•°+ç»“æœä¿¡æ¯"""
    try:
        empty_dirs = await recursive_collect_empty_dirs(token, base_url, target_dir)
        if not empty_dirs:
            return 0, "âœ… æœªæ‰¾åˆ°ç©ºæ–‡ä»¶å¤¹"

        total_deleted = 0
        error_messages = []
        for dir_path in empty_dirs:
            try:
                remove_url = base_url.rstrip('/') + "/api/fs/remove"
                headers = {"Authorization": token, "Content-Type": "application/json"}
                delete_payload = {
                    "dir": os.path.dirname(dir_path),
                    "names": [os.path.basename(dir_path)]
                }
                response = requests.post(remove_url, json=delete_payload, headers=headers, timeout=30)
                if response.status_code == 200:
                    result = response.json()
                    if result.get("code") == 200:
                        total_deleted += 1
                        logger.debug(f"æˆåŠŸåˆ é™¤ç©ºæ–‡ä»¶å¤¹: {dir_path}")
                    else:
                        error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                        error_messages.append(f"æ–‡ä»¶å¤¹ {os.path.basename(dir_path)}: {error_msg}")
                else:
                    response.raise_for_status()
            except requests.exceptions.RequestException as e:
                error_messages.append(f"æ–‡ä»¶å¤¹ {os.path.basename(dir_path)}: ç½‘ç»œé”™è¯¯")
            except Exception as e:
                error_messages.append(f"æ–‡ä»¶å¤¹ {os.path.basename(dir_path)}: {str(e)}")

        if error_messages:
            return total_deleted, (
                f"âŒ éƒ¨åˆ†åˆ é™¤å¤±è´¥ (æˆåŠŸ {total_deleted} æ–‡ä»¶å¤¹)\n"
                f"é”™è¯¯({min(len(error_messages), 3)}/{len(error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )

        return total_deleted, f"âœ… æˆåŠŸåˆ é™¤ {total_deleted} ä¸ªç©ºæ–‡ä»¶å¤¹"

    except Exception as e:
        logger.error(f"æ¸…ç†ç©ºæ–‡ä»¶å¤¹å¼‚å¸¸: {str(e)}", exc_info=True)
        return 0, f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"


async def cleanup_small_files(token: str, base_url: str, target_dir: str) -> tuple[int, str]:
    if SIZE_THRESHOLD == 0:
        return 0, "âœ… å°æ–‡ä»¶æ¸…ç†åŠŸèƒ½æœªå¯ç”¨"
    try:
        from collections import defaultdict
        import os
        from urllib.parse import quote

        logger.info(f"å¼€å§‹æ¸…ç†ç›®å½•: {target_dir}")
        files_to_delete = await recursive_collect_files(token, base_url, target_dir)

        if not files_to_delete:
            return 0, "âœ… æœªæ‰¾åˆ°å°äºæŒ‡å®šå¤§å°çš„æ–‡ä»¶"

        # æŒ‰çˆ¶ç›®å½•åˆ†ç»„æ–‡ä»¶
        dir_files = defaultdict(list)
        for abs_path in files_to_delete:
            parent_dir = os.path.dirname(abs_path)
            file_name = os.path.basename(abs_path)
            dir_files[parent_dir].append(file_name)

        total_deleted_files = 0
        file_error_messages = []

        # åˆ†ç›®å½•æ‰¹é‡åˆ é™¤
        for parent_dir, file_names in dir_files.items():
            try:
                remove_url = base_url.rstrip('/') + "/api/fs/remove"
                headers = {"Authorization": token, "Content-Type": "application/json"}

                # URLç¼–ç å¤„ç†ï¼ˆå…¼å®¹ç‰¹æ®Šå­—ç¬¦ï¼‰
                encoded_dir = quote(parent_dir, safe='')
                encoded_names = [quote(name, safe='') for name in file_names]

                delete_payload = {
                    "dir": parent_dir,
                    "names": file_names
                }

                response = requests.post(remove_url, json=delete_payload, headers=headers, timeout=30)

                if response.status_code == 200:
                    result = response.json()
                    if result.get("code") == 200:
                        deleted = len(file_names)
                        total_deleted_files += deleted
                        logger.debug(f"æˆåŠŸåˆ é™¤ {deleted} ä¸ªæ–‡ä»¶äº {parent_dir}")
                    else:
                        # è®°å½•æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                        error_msg = f"ç›®å½• {os.path.basename(parent_dir)}: API è¿”å›é”™è¯¯ç  {result.get('code')}ï¼Œæ¶ˆæ¯: {result.get('message')}"
                        file_error_messages.append(error_msg)
                else:
                    response.raise_for_status()

            except requests.exceptions.RequestException as e:
                error_msg = f"ç›®å½• {os.path.basename(parent_dir)}: ç½‘ç»œé”™è¯¯ - {str(e)}"
                file_error_messages.append(error_msg)
            except Exception as e:
                error_msg = f"ç›®å½• {os.path.basename(parent_dir)}: {str(e)}"
                file_error_messages.append(error_msg)

        # æ¸…ç†å®Œå°æ–‡ä»¶åï¼Œç»§ç»­æ¸…ç†ç©ºæ–‡ä»¶å¤¹
        total_deleted_dirs, dir_msg = await cleanup_empty_dirs(token, base_url, target_dir)

        # ç”Ÿæˆç»“æœä¿¡æ¯
        if file_error_messages and total_deleted_files == 0 and total_deleted_dirs == 0:
            return 0, (
                f"âŒ éƒ¨åˆ†åˆ é™¤å¤±è´¥ (æˆåŠŸ 0 ä¸ªæ–‡ä»¶å’Œ 0 ä¸ªç©ºæ–‡ä»¶å¤¹)\n"
                f"é”™è¯¯({min(len(file_error_messages), 3)}/{len(file_error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in file_error_messages[:3]])
            )

        if total_deleted_files > 0 or total_deleted_dirs > 0:
            if total_deleted_dirs > 0:
                if file_error_messages:
                    return total_deleted_files, (
                        f"âœ… éƒ¨åˆ†æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼Œä½†æˆåŠŸåˆ é™¤ {total_deleted_files} ä¸ªæ–‡ä»¶å’Œ {total_deleted_dirs} ä¸ªç©ºæ–‡ä»¶å¤¹\n"
                        f"æ–‡ä»¶åˆ é™¤é”™è¯¯({min(len(file_error_messages), 3)}/{len(file_error_messages)}):\n" +
                        "\n".join([f"â€¢ {msg}" for msg in file_error_messages[:3]])
                    )
                else:
                    return total_deleted_files, f"âœ… æˆåŠŸåˆ é™¤ {total_deleted_files} ä¸ªæ–‡ä»¶å’Œ {total_deleted_dirs} ä¸ªç©ºæ–‡ä»¶å¤¹"
            else:
                if file_error_messages:
                    return total_deleted_files, (
                        f"âœ… éƒ¨åˆ†æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼Œä½†æˆåŠŸåˆ é™¤ {total_deleted_files} ä¸ªæ–‡ä»¶\n"
                        f"æ–‡ä»¶åˆ é™¤é”™è¯¯({min(len(file_error_messages), 3)}/{len(file_error_messages)}):\n" +
                        "\n".join([f"â€¢ {msg}" for msg in file_error_messages[:3]])
                    )
                else:
                    return total_deleted_files, f"âœ… æˆåŠŸåˆ é™¤ {total_deleted_files} ä¸ªæ–‡ä»¶ã€‚{dir_msg}"
        else:
            return 0, f"âœ… æœªæ‰¾åˆ°å°äºæŒ‡å®šå¤§å°çš„æ–‡ä»¶ï¼Œ{dir_msg}"

    except Exception as e:
        logger.error(f"æ¸…ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        return 0, f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"


async def find_download_directory(token: str, base_url: str, parent_dir: str, original_code: str) -> tuple[list[str] | None, str | None]:
    """è¿”å›æ‰€æœ‰åŒ¹é…çš„ç›®å½•åˆ—è¡¨"""
    logger.info(f"åœ¨ç›®å½• '{parent_dir}' ä¸­æœç´¢ç•ªå· '{original_code}'...")
    list_url = base_url.rstrip('/') + "/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}

    try:
        # è·¯å¾„æ ‡å‡†åŒ–å¤„ç†
        parent_dir = parent_dir.strip().replace('\\', '/').rstrip('/')
        if not parent_dir.startswith('/'):
            parent_dir = f'/{parent_dir}'

        list_payload = {"path": parent_dir, "page": 1, "per_page": 0}
        response = requests.post(list_url, json=list_payload, headers=headers, timeout=20)
        response.raise_for_status()
        list_result = response.json()

        if list_result.get("code") != 200:
            return None, f"ç›®å½•åˆ—è¡¨å¤±è´¥: {list_result.get('message', 'æœªçŸ¥é”™è¯¯')}"

        content = list_result.get("data", {}).get("content", [])
        target_pattern = re.sub(r'[^a-zA-Z0-9]', '', original_code).lower()
        possible_matches = []

        for item in content:
            if item.get("is_dir"):
                dir_name = item.get("name", "").strip()
                normalized_dir = re.sub(r'[^a-zA-Z0-9]', '', dir_name).lower()
                if normalized_dir.startswith(target_pattern):
                    full_path = f"{parent_dir.rstrip('/')}/{dir_name}".replace('//', '/')
                    possible_matches.append(full_path)
                    logger.debug(f"æ‰¾åˆ°å€™é€‰ç›®å½•: {full_path}")

        return possible_matches, None

    except Exception as e:
        logger.error(f"ç›®å½•æœç´¢å¼‚å¸¸: {str(e)}")
        return None, f"ç›®å½•æœç´¢å¤±è´¥: {str(e)}"


# --- Telegram å‘½ä»¤å¤„ç†å‡½æ•° ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_id = update.effective_user.id
    if user_id not in ALLOWED_USER_IDS:
        await update.message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    await update.message.reply_text(
        'æ¬¢è¿ä½¿ç”¨ JAV ä¸‹è½½æœºå™¨äººï¼\n'
        'ç›´æ¥å‘é€ç•ªå·ï¼ˆå¦‚ ABC-123ï¼‰æˆ–ç£åŠ›é“¾æ¥ï¼Œæˆ‘ä¼šå¸®ä½ æ·»åŠ åˆ° Alist ç¦»çº¿ä¸‹è½½ã€‚\n'
        '/help æŸ¥çœ‹å¸®åŠ©ã€‚'
    )


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_id = update.effective_user.id
    if user_id not in ALLOWED_USER_IDS:
        await update.message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    await update.message.reply_text(
        'ä½¿ç”¨æ–¹æ³•ï¼š\n'
        '1. ç›´æ¥å‘é€ç•ªå·ï¼ˆä¾‹å¦‚ï¼š`ABC-123`, `IPX-888`ï¼‰\n'
        '2. ç›´æ¥å‘é€ç£åŠ›é“¾æ¥ï¼ˆä»¥ `magnet:?` å¼€å¤´ï¼‰\n\n'
        '3. æ¸…ç†åŠŸèƒ½ï¼š\n'
        '   - `/clean <ç•ªå·>` æ¸…ç†è¯¥ç•ªå·å¯¹åº”çš„ä¸‹è½½ç›®å½•\n'
        '   - `/clean /` é€’å½’æ¸…ç†æ‰€æœ‰ä¸‹è½½ç›®å½•ï¼ˆè°¨æ…ä½¿ç”¨ï¼ï¼‰\n\n'
        '4. åˆ·æ–°åŠŸèƒ½ï¼š\n'
        '   - `/refresh` åˆ·æ–° Alist æ–‡ä»¶åˆ—è¡¨\n\n'
        f'å½“å‰é…ç½®çš„ä¸‹è½½æ ¹ç›®å½•: `{OFFLINE_DOWNLOAD_DIR}`',
        parse_mode='Markdown'
    )


FANHAO_REGEX = re.compile(
    r'^[A-Za-z]{2,5}[-_ ]?\d{2,5}(?:[-_ ]?[A-Za-z])?$',
    re.IGNORECASE
)


# æ–°å¢ï¼šå¤„ç†å•æ¡è¾“å…¥çš„å‡½æ•°
async def handle_single_entry(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str, entry: str):
    chat_id = update.effective_chat.id
    loop = asyncio.get_event_loop()
    processing_msg = None

    try:
        if entry.startswith("magnet:?"):
            logger.info(f"æ”¶åˆ°ç£åŠ›é“¾æ¥: {entry[:50]}...")
            processing_msg = await update.message.reply_text("ğŸ”— æ”¶åˆ°ç£åŠ›é“¾æ¥ï¼Œå‡†å¤‡æ·»åŠ ...")
            success, result_msg = await add_magnet(context, token, entry)
        elif FANHAO_REGEX.match(entry):
            logger.info(f"æ”¶åˆ°å¯èƒ½çš„ç•ªå·: {entry}")
            processing_msg = await update.message.reply_text(f"ğŸ” æ­£åœ¨æœç´¢ç•ªå·: {entry}...")
            await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

            # åŒæ­¥å‡½æ•°è½¬å¼‚æ­¥æ‰§è¡Œ
            magnet, error_msg = await loop.run_in_executor(
                None, lambda: get_magnet(entry, SEARCH_URL)
            )

            if not magnet:
                await processing_msg.edit_text(f"âŒ æœç´¢å¤±è´¥: {error_msg}")
                return

            await processing_msg.edit_text(f"âœ… å·²æ‰¾åˆ°ç£åŠ›é“¾æ¥ï¼Œæ­£åœ¨æ·»åŠ åˆ° Alist...")
            success, result_msg = await add_magnet(context, token, magnet)
        else:
            await update.message.reply_text("æ— æ³•è¯†åˆ«çš„æ¶ˆæ¯æ ¼å¼ã€‚è¯·å‘é€ç•ªå·ï¼ˆå¦‚ ABC-123ï¼‰æˆ–ç£åŠ›é“¾æ¥ã€‚")
            return

        if processing_msg:
            await processing_msg.edit_text(result_msg)
        else:
            await update.message.reply_text(result_msg)

        if success:
            await asyncio.sleep(3)
            await refresh_command(update, context)

    except Exception as e:
        logger.error(f"å¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)[:100]}"
        if processing_msg:
            await processing_msg.edit_text(error_msg)
        else:
            await update.message.reply_text(error_msg)


# æ–°å¢ï¼šå¤„ç†æ‰¹é‡è¾“å…¥çš„å‡½æ•°
async def handle_batch_entries(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str, entries: list[str]):
    chat_id = update.effective_chat.id
    loop = asyncio.get_running_loop()
    progress_msg = await update.message.reply_text(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† {len(entries)} ä¸ªä»»åŠ¡...")
    results = []
    BATCH_DELAY = 0.8

    for idx, entry in enumerate(entries, 1):
        try:
            # æ›´æ–°è¿›åº¦æ¶ˆæ¯
            success_count = sum(1 for res in results if res[1])
            await progress_msg.edit_text(
                f"â³ å¤„ç†è¿›åº¦: {idx}/{len(entries)}\n"
                f"å½“å‰é¡¹: {entry[:15]}...\n"
                f"æˆåŠŸ: {success_count} å¤±è´¥: {len(results)-success_count}"
            )

            # å¤„ç†é€»è¾‘
            if entry.startswith("magnet:?"):
                success, msg = await add_magnet(context, token, entry)
                results.append((entry, success, msg))
            elif FANHAO_REGEX.match(entry):
                magnet, error = await loop.run_in_executor(None, lambda: get_magnet(entry, SEARCH_URL))
                if magnet:
                    success, msg = await add_magnet(context, token, magnet)
                    results.append((entry, success, msg))
                else:
                    results.append((entry, False, f"æœç´¢å¤±è´¥: {error}"))
            else:
                results.append((entry, False, "æ ¼å¼é”™è¯¯"))

            await asyncio.sleep(BATCH_DELAY)

        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¼‚å¸¸: {entry} - {str(e)}")
            results.append((entry, False, f"å¤„ç†å¼‚å¸¸: {str(e)[:50]}"))
            await asyncio.sleep(BATCH_DELAY * 2)

    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    success_count = sum(1 for res in results if res[1])
    report = [
        f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ ({success_count}/{len(entries)})",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        *[f"{'ğŸŸ¢' if res[1] else 'ğŸ”´'} {res[0][:20]}... | {res[2][:30]}"
          for res in results[:10]],  # æ˜¾ç¤ºå‰10æ¡ç»“æœ
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        f"æˆåŠŸ: {success_count} æ¡ | å¤±è´¥: {len(entries)-success_count} æ¡"
    ]
    if len(results) > 10:
        report.insert(3, f"ï¼ˆä»…æ˜¾ç¤ºå‰10æ¡ç»“æœï¼Œå…±{len(entries)}æ¡ï¼‰")

    await progress_msg.edit_text("\n".join(report))
    await context.bot.send_message(
        chat_id=chat_id,
        text="ğŸ’¡ æç¤ºï¼šä½¿ç”¨ /clean / å‘½ä»¤å¯ä»¥æ¸…ç†æ‰€æœ‰åƒåœ¾æ–‡ä»¶",
        reply_to_message_id=progress_msg.message_id
    )

    if success_count > 0:
        await asyncio.sleep(3)
        await refresh_command(update, context)


@restricted
async def process_message(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    message_text = update.message.text.strip()
    # åˆ†å‰²å¤šè¡Œè¾“å…¥å¹¶è¿‡æ»¤ç©ºè¡Œ
    entries = [line.strip() for line in message_text.split('\n') if line.strip()]
    if not entries:
        await update.message.reply_text("âš ï¸ è¾“å…¥å†…å®¹ä¸ºç©ºï¼Œè¯·å‘é€ç•ªå·æˆ–ç£åŠ›é“¾æ¥ã€‚")
        return

    if len(entries) == 1:
        await handle_single_entry(update, context, token, entries[0])
    else:
        await handle_batch_entries(update, context, token, entries)


@restricted
async def clean_command(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """è‡ªåŠ¨æ¸…ç†æ‰€æœ‰åŒ¹é…ç›®å½•ï¼ˆå¸¦å®æ—¶è¿›åº¦ï¼‰"""
    if SIZE_THRESHOLD == 0:
        await update.message.reply_text("âœ… å°æ–‡ä»¶æ¸…ç†åŠŸèƒ½æœªå¯ç”¨")
        return
    if not context.args:
        await update.message.reply_text("è¯·æä¾›æ¸…ç†å‚æ•°ï¼š/clean <ç•ªå·> æˆ– /clean /")
        return

    target = context.args[0].strip()
    chat_id = update.effective_chat.id
    processing_msg = await update.message.reply_text(f"ğŸ§¹ å¼€å§‹æ¸…ç†ä»»åŠ¡ï¼ˆç›®æ ‡: {target}ï¼‰...")

    try:
        if target == "/":
            # å…¨ç›®å½•æ¸…ç†é€»è¾‘
            deleted_files, msg = await cleanup_small_files(token, BASE_URL, OFFLINE_DOWNLOAD_DIR)
            final_text = f"å…¨å±€æ¸…ç†å®Œæˆ\n{msg}"
            await processing_msg.edit_text(final_text)
            return

        # è·å–æ‰€æœ‰åŒ¹é…ç›®å½•
        directories, find_error = await find_download_directory(token, BASE_URL, OFFLINE_DOWNLOAD_DIR, target)
        if not directories:
            await processing_msg.edit_text(f"âŒ æ¸…ç†å¤±è´¥: {find_error}")
            return

        logger.info(f"æ‰¾åˆ° {len(directories)} ä¸ªåŒ¹é…ç›®å½•ï¼Œå¼€å§‹æ‰¹é‡æ¸…ç†...")

        success_dirs = 0
        total_files = 0
        total_dirs = len(directories)
        error_messages = []

        # æ¸…ç†æ‰€æœ‰åŒ¹é…ç›®å½•
        for idx, dir_path in enumerate(directories, 1):
            await processing_msg.edit_text(
                f"ğŸ§¹ æ­£åœ¨æ¸…ç† ({idx}/{total_dirs}): {os.path.basename(dir_path)}..."
            )
            deleted, msg = await cleanup_small_files(token, BASE_URL, dir_path)
            if deleted > 0:
                success_dirs += 1
                total_files += deleted
            if 'âŒ' in msg:
                error_messages.append(msg)

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        zero_dirs_count = total_dirs - success_dirs - len(error_messages)
        if success_dirs > 0 and zero_dirs_count == 0 and len(error_messages) == 0:
            final_text = (
                f"âœ… æ¸…ç†å®Œæˆï¼å…±æ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚"
            )
        elif success_dirs > 0 and zero_dirs_count == 0 and len(error_messages) > 0:
            final_text = (
                f"âœ… éƒ¨åˆ†æ¸…ç†å®Œæˆï¼æˆåŠŸæ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚\n"
                f"âŒ ä»¥ä¸‹ç›®å½•æ¸…ç†å¤±è´¥ ({len(error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )
        elif success_dirs > 0 and zero_dirs_count > 0 and len(error_messages) == 0:
            final_text = (
                f"âœ… éƒ¨åˆ†æ¸…ç†å®Œæˆï¼æˆåŠŸæ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚\n"
                f"âš ï¸ ä»¥ä¸‹ç›®å½•æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶ ({zero_dirs_count}):\n" +
                "\n".join([os.path.basename(d) for d in directories if d not in [d for _, msg in zip(directories, msg) if 'âœ…' in msg]])
            )
        elif success_dirs > 0 and zero_dirs_count > 0 and len(error_messages) > 0:
            final_text = (
                f"âœ… éƒ¨åˆ†æ¸…ç†å®Œæˆï¼æˆåŠŸæ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚\n"
                f"âš ï¸ ä»¥ä¸‹ç›®å½•æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶ ({zero_dirs_count}):\n" +
                "\n".join([os.path.basename(d) for d in directories if d not in [d for _, msg in zip(directories, msg) if 'âœ…' in msg]]) +
                f"\nâŒ ä»¥ä¸‹ç›®å½•æ¸…ç†å¤±è´¥ ({len(error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )
        elif success_dirs == 0 and zero_dirs_count == 0 and len(error_messages) > 0:
            final_text = (
                f"âŒ æ¸…ç†å¤±è´¥ï¼æœªæˆåŠŸæ¸…ç†ä»»ä½•ç›®å½•ã€‚\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )
        elif success_dirs == 0 and zero_dirs_count > 0 and len(error_messages) == 0:
            final_text = (
                f"âš ï¸ æ‰€æœ‰ç›®å½•å‡æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶ ({total_dirs})ã€‚"
            )
        elif success_dirs == 0 and zero_dirs_count > 0 and len(error_messages) > 0:
            final_text = (
                f"âŒ æ¸…ç†å¤±è´¥ï¼æœªæˆåŠŸæ¸…ç†ä»»ä½•ç›®å½•ã€‚\n"
                f"âš ï¸ ä»¥ä¸‹ç›®å½•æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶ ({zero_dirs_count}):\n" +
                "\n".join([os.path.basename(d) for d in directories if d not in [d for _, msg in zip(directories, msg) if 'âœ…' in msg]]) +
                f"\nâŒ ä»¥ä¸‹ç›®å½•æ¸…ç†å¤±è´¥ ({len(error_messages)}):\n" +
                "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
            )
        else:
            final_text = (
                f"âœ… éƒ¨åˆ†æ¸…ç†å®Œæˆï¼æˆåŠŸæ¸…ç† {total_files} ä¸ªå°æ–‡ä»¶ï¼Œæ¶‰åŠ {success_dirs} ä¸ªç›®å½•ã€‚"
            )

        await processing_msg.edit_text(final_text)

    except Exception as e:
        logger.error(f"æ¸…ç†å‘½ä»¤å¼‚å¸¸: {str(e)}", exc_info=True)
        await processing_msg.edit_text(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)[:50]}")


@restricted
async def refresh_command(update: Update, context: ContextTypes.DEFAULT_TYPE, *, token: str) -> None:
    """å‘é€åˆ·æ–°è¯·æ±‚ä»¥åˆ·æ–° Alist"""
    refresh_url = BASE_URL.rstrip('/') + "/api/fs/list"
    headers = {
        "Authorization": token,
        "Content-Type": "application/json"
    }
    payload = {"path": "/", "page": 1, "per_page": 0}
    chat_id = update.effective_chat.id
    processing_msg = await update.message.reply_text("ğŸ”„ æ­£åœ¨åˆ·æ–° Alist...")

    try:
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(
            None, lambda: requests.post(refresh_url, json=payload, headers=headers, timeout=30)
        )
        response.raise_for_status()
        result = response.json()

        if result.get("code") == 200:
            await processing_msg.edit_text("âœ… Alist åˆ·æ–°æˆåŠŸï¼")
        else:
            error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
            await processing_msg.edit_text(f"âŒ åˆ·æ–°å¤±è´¥: {error_msg}")

    except requests.exceptions.RequestException as e:
        logger.error(f"åˆ·æ–° Alist æ—¶å‡ºé”™: {str(e)}")
        await processing_msg.edit_text(f"âŒ åˆ·æ–°å¤±è´¥: ç½‘ç»œé”™è¯¯ ({str(e)[:50]})")
    except Exception as e:
        logger.error(f"åˆ·æ–° Alist æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}", exc_info=True)
        await processing_msg.edit_text(f"âŒ åˆ·æ–°å¤±è´¥: æœªçŸ¥é”™è¯¯ ({str(e)[:50]})")


# --- è‡ªåŠ¨æ¸…ç†å®šæ—¶ä»»åŠ¡ ---
async def auto_clean(context: ContextTypes.DEFAULT_TYPE):
    if CLEAN_INTERVAL_MINUTES == 0 or SIZE_THRESHOLD == 0:
        logger.info("è‡ªåŠ¨æ¸…ç†ä»»åŠ¡æœªå¯ç”¨")
        return
    token = await get_token(context)
    if not token:
        logger.error("æ— æ³•è·å– Alist tokenï¼Œè‡ªåŠ¨æ¸…ç†ä»»åŠ¡å¤±è´¥ã€‚")
        return

    chat_id = list(ALLOWED_USER_IDS)[0]  # å‡è®¾ä½¿ç”¨ç¬¬ä¸€ä¸ªå…è®¸çš„ç”¨æˆ· ID å‘é€ç»“æœ
    processing_msg = await context.bot.send_message(chat_id=chat_id, text="ğŸ§¹ å¼€å§‹è‡ªåŠ¨æ¸…ç†ä»»åŠ¡...")

    try:
        deleted_files, msg = await cleanup_small_files(token, BASE_URL, OFFLINE_DOWNLOAD_DIR)
        final_text = f"è‡ªåŠ¨æ¸…ç†å®Œæˆ\n{msg}"
        await processing_msg.edit_text(final_text)
    except Exception as e:
        logger.error(f"è‡ªåŠ¨æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {str(e)}", exc_info=True)
        error_text = [
            "âŒ è‡ªåŠ¨æ¸…ç†è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯",
            f"é”™è¯¯ç±»å‹: {type(e).__name__}",
            f"è¯¦ç»†ä¿¡æ¯: {str(e)}"
        ]
        await processing_msg.edit_text("\n".join(error_text))


# --- ä¸»å‡½æ•° ---
def main() -> None:
    """å¯åŠ¨æœºå™¨äºº"""
    application = Application.builder().token(TELEGRAM_TOKEN).build()

    # æ³¨å†Œå‘½ä»¤å¤„ç†ç¨‹åº
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("clean", clean_command))
    application.add_handler(CommandHandler("refresh", refresh_command))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, process_message))

    # å¯åŠ¨è‡ªåŠ¨æ¸…ç†ä»»åŠ¡
    job_queue = application.job_queue
    job_queue.run_repeating(auto_clean, interval=CLEAN_INTERVAL_MINUTES * 60, first=0)

    # å¯åŠ¨æœºå™¨äºº
    application.run_polling()


if __name__ == "__main__":
    main()
